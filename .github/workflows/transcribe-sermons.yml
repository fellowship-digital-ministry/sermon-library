name: Transcribe and Embed Sermons

on:
  schedule:
    # Run every day at 7AM UTC
    - cron: '0 7 * * *'
  workflow_dispatch:  # Allows manual triggering

# Add permissions block
permissions:
  contents: write

jobs:
  transcribe:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Full history for commits
          token: ${{ secrets.PAT_TOKEN }}  # Use a Personal Access Token
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install required dependencies
          pip install pandas openai tqdm requests
          # Install the correct Pinecone package
          pip uninstall -y pinecone-client
          pip install pinecone
      
      - name: Configure Git
        run: |
          git config --global user.name "Sermon Transcription Bot"
          git config --global user.email "fellowship-digital-ministry@proton.me"
      
      - name: Create Tracking Script
        run: |
          # Create a script to just track new sermon videos without downloading
          cat > track_sermons.py << 'EOF'
          import argparse
          import csv
          import json
          import logging
          import os
          import re
          import requests
          import sys
          from datetime import datetime

          # Setup logging
          logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
          logger = logging.getLogger()

          def load_video_list(csv_path):
              """Load existing video database"""
              videos = {}
              default_columns = [
                  "video_id", "title", "description", "publish_date", 
                  "duration", "view_count", "like_count", "url", "thumbnail",
                  "processing_status", "processing_date", "transcript_path", "embeddings_status",
                  "embeddings_date", "embeddings_count"
              ]
              
              if not os.path.exists(csv_path):
                  logger.info(f"CSV file {csv_path} does not exist, will create a new one")
                  return videos, default_columns
              
              try:
                  with open(csv_path, 'r', encoding='utf-8') as f:
                      reader = csv.DictReader(f)
                      columns = reader.fieldnames or default_columns
                      for row in reader:
                          if 'video_id' in row and row['video_id']:
                              videos[row['video_id']] = row
                  
                  logger.info(f"Loaded {len(videos)} videos from {csv_path}")
                  return videos, columns
              except Exception as e:
                  logger.error(f"Error loading CSV file: {e}")
                  return {}, default_columns

          def save_video_list(videos, columns, csv_path):
              """Save videos dictionary to CSV file with backup"""
              try:
                  # Make a backup of the existing file
                  if os.path.exists(csv_path):
                      backup_file = f"{csv_path}.bak"
                      import shutil
                      shutil.copy(csv_path, backup_file)
                      logger.info(f"Created backup at {backup_file}")
                  
                  # Ensure directory exists
                  os.makedirs(os.path.dirname(csv_path), exist_ok=True)
                  
                  with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                      writer = csv.DictWriter(f, fieldnames=columns)
                      writer.writeheader()
                      for video_data in videos.values():
                          # Ensure all columns exist in each row
                          row = {col: video_data.get(col, "") for col in columns}
                          writer.writerow(row)
                  
                  logger.info(f"Saved {len(videos)} videos to {csv_path}")
                  
              except Exception as e:
                  logger.error(f"Error saving to CSV file: {e}")

          def fetch_channel_videos(channel_id, api_key, max_videos=5):
              """Fetch videos from a channel using the YouTube API"""
              logger.info(f"Fetching up to {max_videos} videos for channel {channel_id}")
              
              # First, get the list of recent videos
              search_url = "https://www.googleapis.com/youtube/v3/search"
              search_params = {
                  "part": "snippet",
                  "channelId": channel_id,
                  "maxResults": max_videos,
                  "order": "date",
                  "type": "video",
                  "key": api_key
              }
              
              try:
                  response = requests.get(search_url, params=search_params)
                  response.raise_for_status()
                  search_data = response.json()
                  
                  videos = []
                  for item in search_data.get('items', []):
                      video_id = item['id']['videoId']
                      logger.info(f"Found video: {video_id}")
                      
                      # Get more detailed video information
                      video_url = "https://www.googleapis.com/youtube/v3/videos"
                      video_params = {
                          "part": "snippet,contentDetails,statistics",
                          "id": video_id,
                          "key": api_key
                      }
                      
                      video_response = requests.get(video_url, params=video_params)
                      video_response.raise_for_status()
                      video_data = video_response.json()
                      
                      if video_data.get('items'):
                          video_info = video_data['items'][0]
                          snippet = video_info['snippet']
                          statistics = video_info.get('statistics', {})
                          
                          # Parse duration (in ISO 8601 format)
                          duration_str = video_info.get('contentDetails', {}).get('duration', 'PT0S')
                          # Convert ISO duration to seconds (simplified)
                          duration_match = re.search(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
                          hours = int(duration_match.group(1) or 0)
                          minutes = int(duration_match.group(2) or 0)
                          seconds = int(duration_match.group(3) or 0)
                          duration = hours * 3600 + minutes * 60 + seconds
                          
                          # Format publish date
                          publish_date = snippet.get('publishedAt', '')
                          if publish_date:
                              dt = datetime.strptime(publish_date, "%Y-%m-%dT%H:%M:%SZ")
                              publish_date = dt.strftime('%Y%m%d')
                          
                          videos.append({
                              "video_id": video_id,
                              "title": snippet.get('title', ''),
                              "description": snippet.get('description', ''),
                              "publish_date": publish_date,
                              "duration": duration,
                              "view_count": int(statistics.get('viewCount', 0)),
                              "like_count": int(statistics.get('likeCount', 0)),
                              "url": f"https://www.youtube.com/watch?v={video_id}",
                              "thumbnail": snippet.get('thumbnails', {}).get('high', {}).get('url', '')
                          })
                      
                  logger.info(f"Found {len(videos)} videos from YouTube API")
                  return videos
                  
              except requests.exceptions.RequestException as e:
                  logger.error(f"Error fetching videos from YouTube API: {e}")
                  return []

          def main():
              parser = argparse.ArgumentParser(description="Track sermon videos from YouTube channel")
              parser.add_argument("--channel-id", required=True, help="YouTube channel ID")
              parser.add_argument("--api-key", required=True, help="YouTube API key")
              parser.add_argument("--output-csv", required=True, help="Path to output CSV file")
              parser.add_argument("--max", type=int, default=5, help="Maximum number of videos to retrieve")
              args = parser.parse_args()
              
              # Load existing videos from CSV
              existing_videos, columns = load_video_list(args.output_csv)
              
              # Fetch latest videos from YouTube
              recent_videos = fetch_channel_videos(args.channel_id, args.api_key, args.max)
              
              # Check for new videos
              new_videos = []
              for video in recent_videos:
                  video_id = video.get('video_id')
                  if not video_id:
                      continue
                      
                  if video_id not in existing_videos:
                      # This is a new video
                      logger.info(f"Found new video: {video_id} - {video.get('title', 'Unknown Title')}")
                      
                      # Create CSV entry with data from API
                      video_data = video.copy()
                      video_data.update({
                          "processing_status": "needs_audio",  # Special status for videos that need audio
                          "processing_date": "",
                          "transcript_path": "",
                          "embeddings_status": "pending",
                          "embeddings_date": "",
                          "embeddings_count": "0"
                      })
                      
                      # Add any missing columns with empty values
                      for col in columns:
                          if col not in video_data:
                              video_data[col] = ""
                      
                      existing_videos[video_id] = video_data
                      new_videos.append(video_id)
                  
                  elif existing_videos[video_id].get("processing_status") in ["failed", "pending", "", "needs_audio"]:
                      # Update video metadata but keep status
                      logger.info(f"Updating metadata for: {video_id} - {video.get('title', 'Unknown Title')}")
                      
                      # Only update metadata fields, not status fields
                      for key in ["title", "description", "view_count", "like_count", "thumbnail"]:
                          if key in video:
                              existing_videos[video_id][key] = video[key]
              
              # Save updated CSV
              save_video_list(existing_videos, columns, args.output_csv)
              
              if new_videos:
                  logger.info(f"Added {len(new_videos)} new videos to the database")
                  # Create placeholder files for all new videos
                  created_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                  for video_id in new_videos:
                      video = existing_videos[video_id]
                      
                      # Create a placeholder metadata file to track this video
                      metadata_dir = os.path.dirname(args.output_csv)
                      os.makedirs(os.path.join(metadata_dir, "metadata"), exist_ok=True)
                      
                      placeholder_data = {
                          "video_id": video_id,
                          "title": video["title"],
                          "description": video["description"],
                          "publish_date": video["publish_date"],
                          "discovery_date": created_time,
                          "status": "needs_audio",
                          "url": video["url"]
                      }
                      
                      metadata_path = os.path.join(metadata_dir, "metadata", f"{video_id}.json")
                      with open(metadata_path, 'w', encoding='utf-8') as f:
                          json.dump(placeholder_data, f, indent=2)
                      
                      logger.info(f"Created placeholder metadata file for {video_id}")
              
              logger.info("Video tracking complete")
              return 0

          if __name__ == "__main__":
              sys.exit(main())
          EOF
          
          # Make the script executable
          chmod +x track_sermons.py
        shell: bash
      
      - name: Run sermon tracking with YouTube API
        env:
          YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}
        run: |
          # Create output directories
          mkdir -p transcription/data/audio
          
          # Run the tracking script with YouTube API
          cd transcription
          python ../track_sermons.py \
            --channel-id "UCek_LI7dZopFJEvwxDnovJg" \
            --api-key "$YOUTUBE_API_KEY" \
            --output-csv "data/video_list.csv" \
            --max 10
        shell: bash
      
      # First pull latest changes before committing
      - name: Pull latest changes
        run: git pull origin main
        
      - name: Generate CSV Report of Videos Needing Audio
        run: |
          # Create a script to generate a report of videos needing audio
          cat > generate_report.py << 'EOF'
          import csv
          import os
          import sys
          from datetime import datetime

          # Read the video list CSV
          csv_path = sys.argv[1]
          needs_audio = []

          with open(csv_path, 'r', encoding='utf-8') as f:
              reader = csv.DictReader(f)
              for row in reader:
                  if row.get('processing_status') == 'needs_audio':
                      needs_audio.append(row)

          # Sort by publish date, most recent first
          needs_audio.sort(key=lambda x: x.get('publish_date', ''), reverse=True)

          # Generate a markdown report
          report_file = "VIDEOS_NEEDING_AUDIO.md"
          with open(report_file, 'w', encoding='utf-8') as f:
              f.write("# Videos Needing Audio Files\n\n")
              f.write("This report was generated on " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n\n")
              f.write("The following sermon videos have been discovered but need audio files downloaded manually:\n\n")
              
              f.write("| Date | Title | Video ID | Link |\n")
              f.write("|------|-------|----------|------|\n")
              
              for video in needs_audio:
                  date = video.get('publish_date', '')
                  if date:
                      try:
                          date = datetime.strptime(date, '%Y%m%d').strftime('%Y-%m-%d')
                      except:
                          pass
                  
                  title = video.get('title', 'Unknown Title')
                  video_id = video.get('video_id', '')
                  link = video.get('url', f'https://www.youtube.com/watch?v={video_id}')
                  
                  f.write(f"| {date} | {title} | {video_id} | [{video_id}]({link}) |\n")
              
              f.write("\n## How to Download Audio\n\n")
              f.write("1. Download the audio file manually using a service like y2mate.com or similar\n")
              f.write("2. Save the audio file as `[VIDEO_ID].mp3` in the `transcription/data/audio` directory\n")
              f.write("3. Run the transcription script: `python transcription/process_batch.py --video-id [VIDEO_ID]`\n")
              
          print(f"Report generated: {report_file}")
          print(f"Found {len(needs_audio)} videos needing audio")
          EOF
          
          # Run the script
          python generate_report.py transcription/data/video_list.csv
        shell: bash
      
      - name: Commit and push changes
        run: |
          git add transcription/data/video_list.csv
          git add transcription/data/metadata/
          git add VIDEOS_NEEDING_AUDIO.md
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update sermon video database and report [skip ci]"
            git push
          fi