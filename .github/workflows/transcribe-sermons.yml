name: Transcribe and Embed Sermons

on:
  schedule:
    # Run every day at 2AM UTC
    - cron: '0 7 * * *'
  workflow_dispatch:  # Allows manual triggering

# Add permissions block
permissions:
  contents: write

jobs:
  transcribe:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Full history for commits
          token: ${{ secrets.PAT_TOKEN }}  # Use a Personal Access Token
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install FFmpeg
        run: sudo apt-get update && sudo apt-get install -y ffmpeg
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Update to latest yt-dlp to handle current YouTube changes
          pip install --upgrade yt-dlp
          pip install pandas openai tqdm pydub pytube
          # Install the correct Pinecone package - new version
          pip uninstall -y pinecone-client
          pip install pinecone
      
      # Create properly formatted cookies file from GitHub secret
      - name: Setup YouTube Cookies
        run: |
          # Create cookies.txt file with correct Netscape format header
          echo "# Netscape HTTP Cookie File" > $HOME/youtube_cookies.txt
          echo "# https://curl.haxx.se/docs/http-cookies.html" >> $HOME/youtube_cookies.txt
          echo "# This file was generated by libcurl! Edit at your own risk." >> $HOME/youtube_cookies.txt
          echo "" >> $HOME/youtube_cookies.txt  # Add empty line
          
          # Process cookie string from secrets
          # This handles cookies in either Netscape format or Name=Value; format
          if [[ "${{ secrets.YOUTUBE_COOKIES }}" == *"	"* ]]; then
            # Cookies already in Netscape format, just add them
            echo "${{ secrets.YOUTUBE_COOKIES }}" >> $HOME/youtube_cookies.txt
          else
            # Try to convert cookies to Netscape format
            echo "${{ secrets.YOUTUBE_COOKIES }}" | sed 's/;/\n/g' | while read cookie; do
              # Extract domain and path from cookie string
              domain=$(echo "$cookie" | grep -o 'domain=[^;]*' | cut -d= -f2)
              path=$(echo "$cookie" | grep -o 'path=[^;]*' | cut -d= -f2 || echo "/")
              name=$(echo "$cookie" | cut -d= -f1 | xargs)
              value=$(echo "$cookie" | cut -d= -f2- | cut -d\; -f1)
              
              if [ ! -z "$domain" ] && [ ! -z "$name" ]; then
                # Format: domain FLAG path secure_flag expiry name value
                echo "$domain	TRUE	$path	FALSE	2147483647	$name	$value" >> $HOME/youtube_cookies.txt
              fi
            done
          fi
          
          # Make sure the file exists and has content
          if [ ! -s "$HOME/youtube_cookies.txt" ]; then
            echo "Warning: Cookies file is empty. This may cause authentication issues."
            # Create a minimal cookie file to avoid errors
            echo ".youtube.com	TRUE	/	FALSE	2147483647	CONSENT	YES+" >> $HOME/youtube_cookies.txt
          else
            echo "Cookies file created successfully"
            # Print a sample (first line only) to verify format without revealing content
            head -n 4 $HOME/youtube_cookies.txt | tail -n 1 | sed 's/[a-zA-Z0-9]\{10,\}/REDACTED/g'
          fi
          
          # Set proper permissions
          chmod 600 $HOME/youtube_cookies.txt
        shell: bash
      
      - name: Configure Git
        run: |
          git config --global user.name "Sermon Transcription Bot"
          git config --global user.email "fellowship-digital-ministry@proton.me"
      
      - name: Create additional yt-dlp config
        run: |
          # Create a configuration file to help bypass restrictions
          mkdir -p ~/.config/yt-dlp
          cat > ~/.config/yt-dlp/config << 'EOL'
          # yt-dlp configuration
          --user-agent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
          --add-header "Accept-Language:en-US,en;q=0.9"
          --sleep-interval 3
          --max-sleep-interval 6
          --extractor-retries 5
          --fragment-retries 5
          --skip-unavailable-fragments
          --console-title
          --no-check-certificate
          EOL
          
          echo "Created yt-dlp config file:"
          cat ~/.config/yt-dlp/config
        shell: bash
      
      - name: Run sermon monitoring with cookies and fallback options
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          # Pass cookies location to the script
          YOUTUBE_COOKIES: $HOME/youtube_cookies.txt
        run: |
          # Output current time
          date
          echo "Starting sermon monitoring process"
          
          # Modify the monitor_channel.py script to add fallback methods
          cat > patch_monitor.py << 'EOL'
          import sys
          import re

          def patch_file(file_path):
              with open(file_path, 'r') as f:
                  content = f.read()

              # Add pytube import and fallback method
              imports = "import os\nimport csv\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Set, Tuple\n\n# Add pytube for fallback\ntry:\n    from pytube import YouTube\nexcept ImportError:\n    print('pytube not installed, fallback will not be available')\n"
              content = re.sub(r'import os.*?from typing import Dict, List, Optional, Set, Tuple', imports, content, flags=re.DOTALL)

              # Add fallback method
              fallback_method = """
          def download_with_pytube(video_id: str, output_dir: str = "data/audio") -> bool:
              """Use pytube as a fallback to download video audio."""
              try:
                  url = f"https://www.youtube.com/watch?v={video_id}"
                  logger.info(f"Attempting pytube download for {video_id}")
                  yt = YouTube(url)
                  # Get audio stream
                  stream = yt.streams.filter(only_audio=True).first()
                  if not stream:
                      logger.error(f"No audio stream found for {video_id}")
                      return False
                  
                  # Create output directory
                  os.makedirs(output_dir, exist_ok=True)
                  output_path = os.path.join(output_dir, f"{video_id}.mp4")
                  
                  # Download the file
                  stream.download(output_path=output_dir, filename=f"{video_id}.mp4")
                  logger.info(f"Successfully downloaded audio for {video_id} using pytube")
                  
                  # Create minimal metadata
                  return {
                      "video_id": video_id,
                      "title": yt.title or f"Unknown Title ({video_id})",
                      "description": yt.description or "",
                      "upload_date": datetime.now().strftime('%Y%m%d'),
                      "duration": yt.length or 0,
                      "view_count": yt.views or 0,
                      "like_count": 0,
                      "url": url,
                      "thumbnail": yt.thumbnail_url or ""
                  }
              except Exception as e:
                  logger.error(f"Pytube download failed for {video_id}: {e}")
                  return None
          """
              content = content.replace("def load_video_list(csv_path: str)", fallback_method + "\n\ndef load_video_list(csv_path: str)")

              # Modify get_video_details to add fallbacks
              original_method = re.search(r'def get_video_details.*?return None', content, re.DOTALL).group(0)
              enhanced_method = """def get_video_details(video_id: str, cookies_file: Optional[str] = None) -> Optional[Dict]:
              \"\"\"Get detailed metadata for a specific video with enhanced anti-bot detection\"\"\"
              args = [
                  "--dump-json",
                  f"https://www.youtube.com/watch?v={video_id}",
                  "--user-agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                  "--add-header", "Accept-Language:en-US,en;q=0.9",
                  "--sleep-interval", "3", "--max-sleep-interval", "6"
              ]
              
              # Try up to 3 times with increasing delays
              for attempt in range(3):
                  success, output = run_yt_dlp(args, cookies_file)
                  
                  if success:
                      try:
                          video_info = json.loads(output)
                          return video_info
                      except json.JSONDecodeError:
                          logger.error(f"Could not parse JSON for video {video_id}")
                  else:
                      logger.error(f"Failed to get details for video {video_id} (attempt {attempt+1}/3)")
                  
                  # Wait longer between retries
                  wait_time = (attempt + 1) * 5
                  logger.info(f"Waiting {wait_time} seconds before retry")
                  time.sleep(wait_time)
              
              # If all attempts failed, try downloading just the audio without metadata
              logger.warning(f"All metadata attempts failed for {video_id}, trying direct audio download")
              try:
                  audio_args = [
                      "-x",  # Extract audio
                      "--audio-format", "mp3",
                      "--audio-quality", "0",  # Best quality
                      "-o", f"data/audio/{video_id}.%(ext)s",
                      f"https://www.youtube.com/watch?v={video_id}"
                  ]
                  
                  success, _ = run_yt_dlp(audio_args, cookies_file)
                  if success:
                      # Create minimal metadata since we couldn't get full details
                      return {
                          "video_id": video_id,
                          "title": f"Sermon {video_id}",
                          "description": "",
                          "upload_date": datetime.now().strftime('%Y%m%d'),
                          "duration": 0,
                          "view_count": 0,
                          "like_count": 0,
                          "url": f"https://www.youtube.com/watch?v={video_id}",
                          "thumbnail": ""
                      }
              except Exception as e:
                  logger.error(f"Direct audio download failed: {e}")
              
              # Final fallback: try pytube
              pytube_result = download_with_pytube(video_id)
              if pytube_result:
                  return pytube_result
                  
              return None"""
              content = content.replace(original_method, enhanced_method)

              with open(file_path, 'w') as f:
                  f.write(content)

          if __name__ == "__main__":
              if len(sys.argv) > 1:
                  patch_file(sys.argv[1])
                  print(f"Successfully patched {sys.argv[1]}")
              else:
                  print("Please provide a file path to patch")
          EOL
          
          # Apply the patch
          python patch_monitor.py transcription/monitor_channel.py
          
          # Run the monitoring script with improved cookie handling
          cd transcription
          python monitor_channel.py --channel "https://www.youtube.com/@chrismann9821" --process --cleanup --cookies "$HOME/youtube_cookies.txt"
        shell: bash
        continue-on-error: true  # Continue even if there are some issues
      
      # Add a backup method using youtube-dl instead of yt-dlp as a last resort
      - name: Fallback to youtube-dl if needed
        run: |
          # Check if any videos were processed
          if ! grep -q "Found 0 new videos" transcription/channel_monitor.log; then
            echo "yt-dlp approach completed successfully"
          else
            echo "yt-dlp approach failed, trying youtube-dl as fallback"
            # Install youtube-dl
            pip install youtube-dl
            
            # Create a simple script to fetch the latest videos and download audio
            cat > transcription/youtube_dl_fallback.py << 'EOL'
            import os
            import json
            import subprocess
            import csv
            from datetime import datetime

            # Channel ID
            CHANNEL_URL = "https://www.youtube.com/@chrismann9821"
            CSV_PATH = "data/video_list.csv"
            COOKIES_PATH = os.environ.get("YOUTUBE_COOKIES", "")

            def run_youtube_dl(args):
                command = ["youtube-dl"] + args
                if COOKIES_PATH:
                    command.extend(["--cookies", COOKIES_PATH])
                print(f"Running: {' '.join(command)}")
                try:
                    result = subprocess.run(command, text=True, capture_output=True)
                    if result.returncode != 0:
                        print(f"Error: {result.stderr}")
                        return False, result.stderr
                    return True, result.stdout
                except Exception as e:
                    print(f"Exception: {e}")
                    return False, str(e)

            def get_channel_videos():
                # Get latest videos
                args = [
                    "--get-id", "--get-title",
                    "--playlist-end", "5",
                    CHANNEL_URL
                ]
                success, output = run_youtube_dl(args)
                if not success:
                    print("Failed to get videos from channel")
                    return []
                
                # Parse output (alternating lines of id and title)
                lines = output.strip().split('\n')
                videos = []
                for i in range(0, len(lines), 2):
                    if i+1 < len(lines):
                        videos.append({
                            "video_id": lines[i],
                            "title": lines[i+1]
                        })
                return videos

            def load_existing_videos():
                if not os.path.exists(CSV_PATH):
                    return {}
                
                existing = {}
                with open(CSV_PATH, 'r') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        if 'video_id' in row:
                            existing[row['video_id']] = row
                return existing

            def download_audio(video_id):
                # Download audio
                audio_dir = "data/audio"
                os.makedirs(audio_dir, exist_ok=True)
                
                args = [
                    "-x", "--audio-format", "mp3",
                    "-o", f"{audio_dir}/{video_id}.%(ext)s",
                    f"https://www.youtube.com/watch?v={video_id}"
                ]
                
                success, _ = run_youtube_dl(args)
                return success

            def update_csv(existing_videos, new_videos):
                # Get columns
                columns = ["video_id", "title", "description", "publish_date", 
                          "duration", "view_count", "like_count", "url", "thumbnail",
                          "processing_status", "processing_date", "transcript_path", "embeddings_status"]
                
                # Update with new videos
                for video in new_videos:
                    video_id = video["video_id"]
                    if video_id not in existing_videos:
                        # Create new entry
                        existing_videos[video_id] = {
                            "video_id": video_id,
                            "title": video["title"],
                            "description": "",
                            "publish_date": datetime.now().strftime('%Y-%m-%d'),
                            "duration": 0,
                            "view_count": 0,
                            "like_count": 0,
                            "url": f"https://www.youtube.com/watch?v={video_id}",
                            "thumbnail": "",
                            "processing_status": "pending",
                            "processing_date": "",
                            "transcript_path": "",
                            "embeddings_status": "pending"
                        }
                        
                        # Ensure audio is downloaded
                        if download_audio(video_id):
                            print(f"Downloaded audio for {video_id}")
                
                # Write CSV
                with open(CSV_PATH, 'w', newline='') as f:
                    writer = csv.DictWriter(f, fieldnames=columns)
                    writer.writeheader()
                    for video_data in existing_videos.values():
                        row = {col: video_data.get(col, "") for col in columns}
                        writer.writerow(row)
                
                print(f"Updated CSV with {len(new_videos)} new videos")

            # Main process
            def main():
                print("Starting youtube-dl fallback process")
                existing_videos = load_existing_videos()
                channel_videos = get_channel_videos()
                print(f"Found {len(channel_videos)} videos from channel")
                
                if channel_videos:
                    update_csv(existing_videos, channel_videos)
                    print("Fallback process completed")
                else:
                    print("No videos found or error occurred")

            if __name__ == "__main__":
                main()
            EOL
            
            # Run the fallback script
            cd transcription
            python youtube_dl_fallback.py
          fi
        env:
          YOUTUBE_COOKIES: $HOME/youtube_cookies.txt
        shell: bash
        continue-on-error: true
      
      # Add the embedding step here
      - name: Generate embeddings for new transcripts
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
          PINECONE_ENVIRONMENT: ${{ secrets.PINECONE_ENVIRONMENT || 'us-east-1' }}
          PINECONE_INDEX_NAME: ${{ secrets.PINECONE_INDEX_NAME || 'sermon-embeddings' }}
        run: |
          python tools/transcript_to_embeddings.py --skip_existing
      
      # Add this new step to update metadata in Pinecone
      - name: Update Pinecone metadata from JSON files
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          PINECONE_API_KEY: ${{ secrets.PINECONE_API_KEY }}
          PINECONE_ENVIRONMENT: ${{ secrets.PINECONE_ENVIRONMENT || 'us-east-1' }}
          PINECONE_INDEX_NAME: ${{ secrets.PINECONE_INDEX_NAME || 'sermon-embeddings' }}
        run: |
          # Copy the metadata utilities to the tools directory
          cp api/metadata_utils.py tools/
          
          # Run the metadata update script for only recent changes
          # This ensures we only update metadata for newly processed sermons
          python api/update_pinecone_metadata.py --only-recent --days=7
      
      # Clean up cookies after use for security
      - name: Clean up cookies
        run: rm -f $HOME/youtube_cookies.txt
        if: always()  # Always run cleanup even if previous steps fail
      
      # First pull latest changes before committing
      - name: Pull latest changes
        run: git pull origin main
        
      - name: Commit and push changes
        run: |
          git add transcription/data/transcripts/
          git add transcription/data/metadata/
          git add transcription/data/video_list.csv
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Add new sermon transcripts and subtitles [skip ci]"
            git push
          fi